{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Steps\n",
    "\n",
    "- Train on language translation\n",
    "- Train on framework translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'CodeLlamaForSeq2SeqLM' from 'transformers' (c:\\Users\\Ayush\\anaconda3\\Lib\\site-packages\\transformers\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RobertaTokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments, CodeLlamaTokenizer, CodeLlamaForSeq2SeqLM\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Check if CUDA is available and set the device\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'CodeLlamaForSeq2SeqLM' from 'transformers' (c:\\Users\\Ayush\\anaconda3\\Lib\\site-packages\\transformers\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments, CodeLlamaTokenizer, CodeLlamaForSeq2SeqLM\n",
    "from datasets import Dataset\n",
    "\n",
    "# Check if CUDA is available and set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the dataset\n",
    "data_path = './data/raw/samples_dart_javascript.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Prepare the dataset for the Hugging Face `datasets` library\n",
    "df['translation'] = df.apply(lambda row: {'dart': row['dart'], 'javascript': row['javascript']}, axis=1)\n",
    "dataset = Dataset.from_pandas(df[['translation']])\n",
    "\n",
    "# Function to preprocess the data\n",
    "def preprocess_function(examples):\n",
    "    inputs = [f\"translate: {ex['dart']}\" for ex in examples['translation']]\n",
    "    targets = [ex['javascript'] for ex in examples['translation']]\n",
    "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n",
    "    labels = tokenizer(targets, max_length=512, truncation=True, padding=\"max_length\").input_ids\n",
    "\n",
    "    model_inputs['labels'] = labels\n",
    "    return model_inputs\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = CodeLlamaTokenizer.from_pretrained(\"facebook/incoder-1B\")\n",
    "model = CodeLlamaForSeq2SeqLM.from_pretrained(\"facebook/incoder-1B\")\n",
    "\n",
    "# Preprocess the dataset\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    fp16=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    eval_dataset=tokenized_dataset,\n",
    ")\n",
    "\n",
    "# Check if the Trainer is using the right device\n",
    "print(f\"Trainer device: {trainer.args.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f246aac0aac340b6a23d982134c5213f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/159 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5624f7f78c6a488a840f6d271089bcb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.19254176318645477, 'eval_runtime': 428.9199, 'eval_samples_per_second': 0.494, 'eval_steps_per_second': 0.124, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cb5ecfb4c9441b7a28fbbb92c67a926",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0953802615404129, 'eval_runtime': 541.6172, 'eval_samples_per_second': 0.391, 'eval_steps_per_second': 0.098, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eac7f1ebb56949458294113a6b9d4b48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.07500644028186798, 'eval_runtime': 602.5238, 'eval_samples_per_second': 0.352, 'eval_steps_per_second': 0.088, 'epoch': 3.0}\n",
      "{'train_runtime': 4876.6808, 'train_samples_per_second': 0.13, 'train_steps_per_second': 0.033, 'train_loss': 0.4858616762940989, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./models/fine-tuned-model\\\\tokenizer_config.json',\n",
       " './models/fine-tuned-model\\\\special_tokens_map.json',\n",
       " './models/fine-tuned-model\\\\vocab.json',\n",
       " './models/fine-tuned-model\\\\merges.txt',\n",
       " './models/fine-tuned-model\\\\added_tokens.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained('./models/language-fine-tuned-model')\n",
    "tokenizer.save_pretrained('./models/language-fine-tuned-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input code:\n",
      "  double findMedianSortedArrays(List<int> nums1, List<int> nums2) {\n",
      "    List<int> merged = [];\n",
      "    int i = 0, j = 0;\n",
      "  \n",
      "    while (i < nums1.length && j < nums2.length) {\n",
      "      if (nums1[i] < nums2[j]) {\n",
      "        merged.add(nums1[i++]);\n",
      "      } else {\n",
      "        merged.add(nums2[j++]);\n",
      "      }\n",
      "    }\n",
      "  \n",
      "    while (i < nums1.length) {\n",
      "      merged.add(nums1[i++]);\n",
      "    }\n",
      "  \n",
      "    while (j < nums2.length) {\n",
      "      merged.add(nums2[j++]);\n",
      "    }\n",
      "  \n",
      "    int n = merged.length;\n",
      "    if (n % 2 == 0) {\n",
      "      return (merged[n ~/ 2 - 1] + merged[n ~/ 2]) / 2;\n",
      "    } else {\n",
      "      return merged[n ~/ 2].toDouble();\n",
      "    }\n",
      "  }\n",
      "  \n",
      "Translated code:\n",
      "  var findMedianSortedArrays = function(nums1, nums2) {\n",
      "    let merged = [];\n",
      "    let i = 0, j = 0;\n",
      "  \n",
      "    while (i < nums1.length && j < nums2.length) {\n",
      "      if (nums1[i] < nums2[j]) {\n",
      "        merged.push(nums1[i++]);\n",
      "      } else {\n",
      "        merged.push(nums2[j++]);\n",
      "      }\n",
      "    }\n",
      "  \n",
      "    while (i < nums1.length) {\n",
      "      merged.push(nums1[i]);\n",
      "    }\n",
      "  \n",
      "    while (j < nums2.length) {\n",
      "      merged.push(nums1[i++]);\n",
      "    }\n",
      "  \n",
      "    while (i < nums1.length) {\n",
      "      merged.push(nums1[i++]);\n",
      "    }\n",
      "  \n",
      "    while (j < nums2.length) {\n",
      "      merged.push(nums2[j++]);\n",
      "    }\n",
      "  \n",
      "    while (i < nums1.length) {\n",
      "      merged.push(nums1[i++]);\n",
      "    }\n",
      "  \n",
      "    while (j < nums2.length) {\n",
      "      merged.push(nums2[j++]);\n",
      "    }\n",
      "  \n",
      "    while (i < nums1.length) {\n",
      "      merged.push(nums1[i++]);\n",
      "    }\n",
      "  \n",
      "    while (j < nums2.length) {\n",
      "      merged.push(nums2[j++]);\n",
      "    }\n",
      "  \n",
      "    while (i < nums1.length) {\n",
      "      merged.push(nums1[i++]);\n",
      "    }\n",
      "  \n",
      "    while (j < nums2.length) {\n",
      "      merged.push(nums2[j++]);\n",
      "    }\n",
      "  \n",
      "    while (i < nums1.length) {\n",
      "      merged.push(nums1[i++]);\n",
      "    }\n",
      "  \n",
      "    while (j < nums2.length) {\n",
      "      merged.push(nums2[j++]);\n",
      "    }\n",
      "  \n",
      "    while (j < nums2.length) {\n",
      "      merged.push(nums2[j++]);\n",
      "    }\n",
      "  \n",
      "    while (n % 2 === 0) {\n",
      "      return (merged[n ~/ 2]) / 2] + merged[n ~/ 2]) / 2].toDouble(merged\n"
     ]
    }
   ],
   "source": [
    "# Test the fine-tuned model with a sample translation\n",
    "code = df['translation'][3]['dart']\n",
    "input_text = f\"translate: {code}\"\n",
    "print(f\"Input code:\\n{code}\")\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generate the translation\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(input_ids, max_length=512)\n",
    "\n",
    "# Decode the output\n",
    "translated_code = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(f\"Translated code:\\n{translated_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32100, 768)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32100, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32100, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=32100, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the saved model\n",
    "m = T5ForConditionalGeneration.from_pretrained('./models/language-fine-tuned-model').to(device)\n",
    "t = RobertaTokenizer.from_pretrained('./models/language-fine-tuned-model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Framework Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f41a640167f4dbab5fe31e6f5ac81d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/35 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ayush\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "data_path = './data/raw/samples_flutter_react-native.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Prepare the dataset for the Hugging Face `datasets` library\n",
    "df['translation'] = df.apply(lambda row: {'dart': row['flutter'], 'javascript': row['react-native']}, axis=1)\n",
    "dataset = Dataset.from_pandas(df[['translation']])\n",
    "\n",
    "# Preprocess the dataset\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    fp16=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    eval_dataset=tokenized_dataset,\n",
    ")\n",
    "\n",
    "# Check if the Trainer is using the right device\n",
    "print(f\"Trainer device: {trainer.args.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32b19e886f5b49198f89fad754ff2ac0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29305d6a02bf46b19afbaceb1066e96a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8296769857406616, 'eval_runtime': 26.5886, 'eval_samples_per_second': 1.316, 'eval_steps_per_second': 0.338, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dd30c5cb47b475ca06a534d28921911",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5303112268447876, 'eval_runtime': 32.1043, 'eval_samples_per_second': 1.09, 'eval_steps_per_second': 0.28, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5343bf6431e943b39a9c8c1f7b28a11b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.434829592704773, 'eval_runtime': 19.3871, 'eval_samples_per_second': 1.805, 'eval_steps_per_second': 0.464, 'epoch': 3.0}\n",
      "{'train_runtime': 465.7138, 'train_samples_per_second': 0.225, 'train_steps_per_second': 0.058, 'train_loss': 2.1653288382071034, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./models/framework-fine-tuned-model\\\\tokenizer_config.json',\n",
       " './models/framework-fine-tuned-model\\\\special_tokens_map.json',\n",
       " './models/framework-fine-tuned-model\\\\vocab.json',\n",
       " './models/framework-fine-tuned-model\\\\merges.txt',\n",
       " './models/framework-fine-tuned-model\\\\added_tokens.json')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained('./models/framework-fine-tuned-model')\n",
    "tokenizer.save_pretrained('./models/framework-fine-tuned-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input code:\n",
      "  import 'package:flutter/material.dart';\n",
      "  import 'package:image_picker/image_picker.dart';\n",
      "  \n",
      "  void main() => runApp(MyApp());\n",
      "  \n",
      "  class MyApp extends StatelessWidget {\n",
      "    @override\n",
      "    Widget build(BuildContext context) {\n",
      "      return MaterialApp(\n",
      "        home: CameraScreen(),\n",
      "      );\n",
      "    }\n",
      "  }\n",
      "  \n",
      "  class CameraScreen extends StatefulWidget {\n",
      "    @override\n",
      "    _CameraScreenState createState() => _CameraScreenState();\n",
      "  }\n",
      "  \n",
      "  class _CameraScreenState extends State<CameraScreen> {\n",
      "    final ImagePicker _picker = ImagePicker();\n",
      "  \n",
      "    void _openCamera() async {\n",
      "      final pickedFile = await _picker.pickImage(source: ImageSource.camera);\n",
      "      if (pickedFile != null) {\n",
      "        // Handle the captured image\n",
      "        print('Image Path: ${pickedFile.path}');\n",
      "      }\n",
      "    }\n",
      "  \n",
      "    @override\n",
      "    Widget build(BuildContext context) {\n",
      "      return Scaffold(\n",
      "        appBar: AppBar(\n",
      "          title: Text('Open Camera Example'),\n",
      "        ),\n",
      "        body: Center(\n",
      "          child: ElevatedButton(\n",
      "            onPressed: _openCamera,\n",
      "            child: Text('Open Camera'),\n",
      "            style: ElevatedButton.styleFrom(\n",
      "              primary: Colors.green,\n",
      "              padding: EdgeInsets.symmetric(horizontal: 30, vertical: 15),\n",
      "              textStyle: TextStyle(fontSize: 20),\n",
      "            ),\n",
      "          ),\n",
      "        ),\n",
      "      );\n",
      "    }\n",
      "  }\n",
      "  \n",
      "Translated code:\n",
      "  import { runApp } from'react';\n",
      "  import { MaterialApp } from'react-native';\n",
      "  import { ImagePicker } from'react-native';\n",
      "  import { Image } from'react-native';\n",
      "  import { Image } from'react-native';\n",
      "  import { Image } from'react-native';\n",
      "  import { Image } from'react-native';\n",
      "  import { Image } from'react-native';\n",
      "  import { const { Image } from'react-native.import { Image } from'react-native-react-native.components.ImageView } from'react-native';\n",
      "  import { ImagePicker } from'react-native';\n",
      "  import { Image } from'react-native';\n",
      "  import { Image } from'react-native';\n",
      "  import { Image } from'react-native';\n",
      "  import { View } from'react-native';\n",
      "  import { StyleSheet} from'react-native.components.ImageView.components.ImageView.View.View.View.View {\n",
      "      const App = {\n",
      "          title: Text('Open Camera Example'),\n",
      "        };\n",
      "        style: {\n",
      "          };\n",
      "          };\n",
      "          style: {\n",
      "          };\n",
      "          style: {\n",
      "          };\n",
      "          style: {\n",
      "          };\n",
      "          style: {\n",
      "          };\n",
      "          };\n",
      "          style: {\n",
      "          };\n",
      "          style: {\n",
      "          };\n",
      "          };\n",
      "  \n",
      "    const Scaffold = {\n",
      "        appBar: AppBar(\n",
      "          title: Text('Open Camera Example'),\n",
      "        style: {\n",
      "        };\n",
      "        };\n",
      "          style: {\n",
      "        };\n",
      "          style: {\n",
      "          };\n",
      "          style: {\n",
      "          };\n",
      "          };\n",
      "          style: {\n",
      "        };\n",
      "          };\n",
      "          };\n",
      "          style: {\n",
      "        };\n",
      "          };\n",
      "          };\n",
      "  const Scaffold = {\n",
      "        appBar: AppBar(\n",
      "          title: Text('Open Camera Example'),\n",
      "        style: {\n",
      "        style: {\n",
      "        style: {\n",
      "        };\n",
      "        style: {\n",
      "        };\n",
      "          style: {\n",
      "          };\n",
      "          };\n",
      "          style: {\n",
      "        };\n",
      "          };\n",
      "          style: {\n"
     ]
    }
   ],
   "source": [
    "# Test the fine-tuned model with a sample translation\n",
    "code = df['translation'][8]['dart']\n",
    "input_text = f\"translate: {code}\"\n",
    "print(f\"Input code:\\n{code}\")\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generate the translation\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(input_ids, max_length=512)\n",
    "\n",
    "# Decode the output\n",
    "translated_code = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(f\"Translated code:\\n{translated_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import subprocess\n",
    "from pygls.client import JsonRPCClient\n",
    "\n",
    "class PyrightClient(JsonRPCClient):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.server_process = None\n",
    "\n",
    "    async def start_server(self):\n",
    "        self.server_process = subprocess.Popen(\n",
    "            ['pyright-langserver', '--stdio'],\n",
    "            stdin=subprocess.PIPE,\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE\n",
    "        )\n",
    "        await self.connect(\n",
    "            self.server_process.stdout,\n",
    "            self.server_process.stdin\n",
    "        )\n",
    "\n",
    "    async def initialize(self):\n",
    "        initialize_params = {\n",
    "            'processId': None,\n",
    "            'rootUri': None,\n",
    "            'capabilities': {},\n",
    "            'trace': 'off',\n",
    "            'workspaceFolders': None\n",
    "        }\n",
    "        response = await self.send_request('initialize', initialize_params)\n",
    "        await self.send_notification('initialized', {})\n",
    "        return response\n",
    "\n",
    "    async def get_completion(self, text, line, character):\n",
    "        completion_params = {\n",
    "            'textDocument': {'uri': 'file:///dummy.py'},\n",
    "            'position': {'line': line, 'character': character},\n",
    "        }\n",
    "        response = await self.send_request('textDocument/completion', completion_params)\n",
    "        return response\n",
    "\n",
    "    async def shutdown(self):\n",
    "        await self.send_request('shutdown', {})\n",
    "        await self.send_notification('exit', {})\n",
    "\n",
    "async def main():\n",
    "    client = PyrightClient()\n",
    "    await client.start_server()\n",
    "    await client.initialize()\n",
    "\n",
    "    code = \"import os\\nos.\"\n",
    "    lines = code.split('\\n')\n",
    "    line = len(lines) - 1\n",
    "    character = len(lines[-1])\n",
    "\n",
    "    completions = await client.get_completion(code, line, character)\n",
    "    print(json.dumps(completions, indent=2))\n",
    "\n",
    "    await client.shutdown()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ayush\\anaconda3\\Lib\\asyncio\\runners.py:186\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(main, debug)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \n\u001b[0;32m    163\u001b[0m \u001b[38;5;124;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;124;03m    asyncio.run(main())\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;66;03m# fail fast with short traceback\u001b[39;00m\n\u001b[1;32m--> 186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    187\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Runner(debug\u001b[38;5;241m=\u001b[39mdebug) \u001b[38;5;28;01mas\u001b[39;00m runner:\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mrun(main)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "asyncio.run(main())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
